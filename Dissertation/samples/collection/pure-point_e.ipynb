{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Point_E Model Scanning Sample</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Point_E Image To PointCloud<h3/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from AIScan import CloudSampler\n",
    "from AIScan.Models import CreateModel\n",
    "from Utils.Data import getPathImages, getPathNPBinaries\n",
    "from Utils.Torch import UseBestTorchDevice\n",
    "\n",
    "from point_e.util.point_cloud import PointCloud\n",
    "from point_e.util.mesh import TriMesh\n",
    "from point_e.util.pc_to_mesh import marching_cubes_mesh\n",
    "from point_e.util.plotting import plot_point_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcPath = '../data/raw/sphere'\n",
    "outputPath = '../data/Point_E/sphere'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device for Torch Model.\n",
      "Loading base40M Model\n",
      "Loading Base Model Checkpoint\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1dd20d104c458485a6393ffcba2468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/162M [00:00<?, ?iB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading upsample Model\n",
      "Loading Base Model Checkpoint\n",
      "Loading base40M Diffusion\n",
      "Loading upsample Diffusion\n"
     ]
    }
   ],
   "source": [
    "# Loading Sampler For Chosen Cloud\n",
    "\n",
    "sampler = CloudSampler.CreateCloudSampler('base1B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 images found in ../data/raw/sphere\n"
     ]
    }
   ],
   "source": [
    "# Loading Images From Provided Source Path (Provided In First Cell)\n",
    "\n",
    "images = getPathImages(srcPath)\n",
    "if (len(images) == 0):\n",
    "    print(\"--__ No Images Found __--\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa67a16b12342c9a71bbc81eeea5314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.64 GiB (GPU 0; 6.00 GiB total capacity; 3.37 GiB already allocated; 650.00 MiB free; 3.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Generate Samples from the Sampler, Using Provided Images\u001b[39;00m\n\u001b[0;32m      3\u001b[0m samples \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m tqdm(sampler\u001b[39m.\u001b[39msample_batch_progressive(batch_size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(images), model_kwargs\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(images\u001b[39m=\u001b[39mimages))):\n\u001b[0;32m      6\u001b[0m     samples \u001b[39m=\u001b[39m x\n\u001b[0;32m      8\u001b[0m \u001b[39mdel\u001b[39;00m images\n",
      "File \u001b[1;32mc:\\Users\\finnd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\notebook.py:259\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[1;32m--> 259\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[0;32m    260\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    261\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m    262\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\finnd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\finnd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\point_e\\diffusion\\sampler.py:163\u001b[0m, in \u001b[0;36mPointCloudSampler.sample_batch_progressive\u001b[1;34m(self, batch_size, model_kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m         internal_batch_size \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m    156\u001b[0m     samples_it \u001b[39m=\u001b[39m diffusion\u001b[39m.\u001b[39mp_sample_loop_progressive(\n\u001b[0;32m    157\u001b[0m         model,\n\u001b[0;32m    158\u001b[0m         shape\u001b[39m=\u001b[39m(internal_batch_size, \u001b[39m*\u001b[39msample_shape[\u001b[39m1\u001b[39m:]),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m         clip_denoised\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip_denoised,\n\u001b[0;32m    162\u001b[0m     )\n\u001b[1;32m--> 163\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m samples_it:\n\u001b[0;32m    164\u001b[0m     samples \u001b[39m=\u001b[39m x[\u001b[39m\"\u001b[39m\u001b[39mpred_xstart\u001b[39m\u001b[39m\"\u001b[39m][:batch_size]\n\u001b[0;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlow_res\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m stage_model_kwargs:\n",
      "File \u001b[1;32mc:\\Users\\finnd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\point_e\\diffusion\\k_diffusion.py:181\u001b[0m, in \u001b[0;36mkarras_sample_progressive\u001b[1;34m(diffusion, model, shape, steps, clip_denoised, progress, model_kwargs, device, sigma_min, sigma_max, rho, sampler, s_churn, s_tmin, s_tmax, s_noise, guidance_scale)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m     guided_denoiser \u001b[39m=\u001b[39m denoiser\n\u001b[1;32m--> 181\u001b[0m \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m sample_fn(\n\u001b[0;32m    182\u001b[0m     guided_denoiser,\n\u001b[0;32m    183\u001b[0m     x_T,\n\u001b[0;32m    184\u001b[0m     sigmas,\n\u001b[0;32m    185\u001b[0m     progress\u001b[39m=\u001b[39mprogress,\n\u001b[0;32m    186\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msampler_args,\n\u001b[0;32m    187\u001b[0m ):\n\u001b[0;32m    188\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(diffusion, GaussianDiffusion):\n\u001b[0;32m    189\u001b[0m         \u001b[39myield\u001b[39;00m diffusion\u001b[39m.\u001b[39munscale_out_dict(obj)\n",
      "File \u001b[1;32mc:\\Users\\finnd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\grad_mode.py:43\u001b[0m, in \u001b[0;36m_DecoratorContextManager._wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m     \u001b[39m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 43\u001b[0m         response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m     45\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m             \u001b[39m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\finnd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\point_e\\diffusion\\k_diffusion.py:265\u001b[0m, in \u001b[0;36msample_heun\u001b[1;34m(denoiser, x, sigmas, progress, s_churn, s_tmin, s_tmax, s_noise)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39mif\u001b[39;00m gamma \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    264\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m eps \u001b[39m*\u001b[39m (sigma_hat\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m-\u001b[39m sigmas[i] \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m\n\u001b[1;32m--> 265\u001b[0m denoised \u001b[39m=\u001b[39m denoiser(x, sigma_hat \u001b[39m*\u001b[39;49m s_in)\n\u001b[0;32m    266\u001b[0m d \u001b[39m=\u001b[39m to_d(x, sigma_hat, denoised)\n\u001b[0;32m    267\u001b[0m \u001b[39myield\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m: x, \u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m: i, \u001b[39m\"\u001b[39m\u001b[39msigma\u001b[39m\u001b[39m\"\u001b[39m: sigmas[i], \u001b[39m\"\u001b[39m\u001b[39msigma_hat\u001b[39m\u001b[39m\"\u001b[39m: sigma_hat, \u001b[39m\"\u001b[39m\u001b[39mpred_xstart\u001b[39m\u001b[39m\"\u001b[39m: denoised}\n",
      "File \u001b[1;32mc:\\Users\\finnd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\point_e\\diffusion\\k_diffusion.py:173\u001b[0m, in \u001b[0;36mkarras_sample_progressive.<locals>.guided_denoiser\u001b[1;34m(x_t, sigma)\u001b[0m\n\u001b[0;32m    171\u001b[0m x_t \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mcat([x_t, x_t], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    172\u001b[0m sigma \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mcat([sigma, sigma], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m--> 173\u001b[0m x_0 \u001b[39m=\u001b[39m denoiser(x_t, sigma)\n\u001b[0;32m    174\u001b[0m cond_x_0, uncond_x_0 \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39msplit(x_0, \u001b[39mlen\u001b[39m(x_0) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    175\u001b[0m x_0 \u001b[39m=\u001b[39m uncond_x_0 \u001b[39m+\u001b[39m guidance_scale \u001b[39m*\u001b[39m (cond_x_0 \u001b[39m-\u001b[39m uncond_x_0)\n",
      "File \u001b[1;32mc:\\Users\\finnd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\point_e\\diffusion\\k_diffusion.py:160\u001b[0m, in \u001b[0;36mkarras_sample_progressive.<locals>.denoiser\u001b[1;34m(x_t, sigma)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdenoiser\u001b[39m(x_t, sigma):\n\u001b[1;32m--> 160\u001b[0m     _, denoised \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdenoise(\n\u001b[0;32m    161\u001b[0m         x_t, sigma, clip_denoised\u001b[39m=\u001b[39;49mclip_denoised, model_kwargs\u001b[39m=\u001b[39;49mmodel_kwargs\n\u001b[0;32m    162\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mreturn\u001b[39;00m denoised\n",
      "File \u001b[1;32mc:\\Users\\finnd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\point_e\\diffusion\\k_diffusion.py:105\u001b[0m, in \u001b[0;36mGaussianToKarrasDenoiser.denoise\u001b[1;34m(self, x_t, sigmas, clip_denoised, model_kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m t \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mtensor(\n\u001b[0;32m    100\u001b[0m     [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigma_to_t(sigma) \u001b[39mfor\u001b[39;00m sigma \u001b[39min\u001b[39;00m sigmas\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()],\n\u001b[0;32m    101\u001b[0m     dtype\u001b[39m=\u001b[39mth\u001b[39m.\u001b[39mlong,\n\u001b[0;32m    102\u001b[0m     device\u001b[39m=\u001b[39msigmas\u001b[39m.\u001b[39mdevice,\n\u001b[0;32m    103\u001b[0m )\n\u001b[0;32m    104\u001b[0m c_in \u001b[39m=\u001b[39m append_dims(\u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m (sigmas\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m, x_t\u001b[39m.\u001b[39mndim)\n\u001b[1;32m--> 105\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdiffusion\u001b[39m.\u001b[39;49mp_mean_variance(\n\u001b[0;32m    106\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, x_t \u001b[39m*\u001b[39;49m c_in, t, clip_denoised\u001b[39m=\u001b[39;49mclip_denoised, model_kwargs\u001b[39m=\u001b[39;49mmodel_kwargs\n\u001b[0;32m    107\u001b[0m )\n\u001b[0;32m    108\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m, out[\u001b[39m\"\u001b[39m\u001b[39mpred_xstart\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\finnd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\point_e\\diffusion\\gaussian_diffusion.py:285\u001b[0m, in \u001b[0;36mGaussianDiffusion.p_mean_variance\u001b[1;34m(self, model, x, t, clip_denoised, denoised_fn, model_kwargs)\u001b[0m\n\u001b[0;32m    283\u001b[0m B, C \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[:\u001b[39m2\u001b[39m]\n\u001b[0;32m    284\u001b[0m \u001b[39massert\u001b[39;00m t\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (B,)\n\u001b[1;32m--> 285\u001b[0m model_output \u001b[39m=\u001b[39m model(x, t, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m    286\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model_output, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    287\u001b[0m     model_output, extra \u001b[39m=\u001b[39m model_output\n",
      "File \u001b[1;32mc:\\Users\\finnd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\finnd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\point_e\\models\\transformer.py:355\u001b[0m, in \u001b[0;36mCLIPImageGridPointDiffusionTransformer.forward\u001b[1;34m(self, x, t, images, embeddings)\u001b[0m\n\u001b[0;32m    352\u001b[0m clip_embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclip_embed(clip_out)\n\u001b[0;32m    354\u001b[0m cond \u001b[39m=\u001b[39m [(t_embed, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime_token_cond), (clip_embed, \u001b[39mTrue\u001b[39;00m)]\n\u001b[1;32m--> 355\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_with_cond(x, cond)\n",
      "File \u001b[1;32mc:\\Users\\finnd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\point_e\\models\\transformer.py:221\u001b[0m, in \u001b[0;36mPointDiffusionTransformer._forward_with_cond\u001b[1;34m(self, x, cond_as_token)\u001b[0m\n\u001b[0;32m    218\u001b[0m     h \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(extra_tokens \u001b[39m+\u001b[39m [h], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    220\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_pre(h)\n\u001b[1;32m--> 221\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(h)\n\u001b[0;32m    222\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_post(h)\n\u001b[0;32m    223\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(extra_tokens):\n",
      "File \u001b[1;32mc:\\Users\\finnd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\finnd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\point_e\\models\\transformer.py:151\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m    150\u001b[0m     \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresblocks:\n\u001b[1;32m--> 151\u001b[0m         x \u001b[39m=\u001b[39m block(x)\n\u001b[0;32m    152\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\finnd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\finnd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\point_e\\models\\transformer.py:113\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> 113\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_1(x))\n\u001b[0;32m    114\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(x))\n\u001b[0;32m    115\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\finnd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\finnd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\point_e\\models\\transformer.py:46\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     45\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_qkv(x)\n\u001b[1;32m---> 46\u001b[0m     x \u001b[39m=\u001b[39m checkpoint(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention, (x,), (), \u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     47\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(x)\n\u001b[0;32m     48\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\finnd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\point_e\\models\\checkpoint.py:27\u001b[0m, in \u001b[0;36mcheckpoint\u001b[1;34m(func, inputs, params, flag)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mif\u001b[39;00m flag:\n\u001b[0;32m     26\u001b[0m     args \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(inputs) \u001b[39m+\u001b[39m \u001b[39mtuple\u001b[39m(params)\n\u001b[1;32m---> 27\u001b[0m     \u001b[39mreturn\u001b[39;00m CheckpointFunction\u001b[39m.\u001b[39;49mapply(func, \u001b[39mlen\u001b[39;49m(inputs), \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     28\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39minputs)\n",
      "File \u001b[1;32mc:\\Users\\finnd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\point_e\\models\\checkpoint.py:39\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[1;34m(ctx, run_function, length, *args)\u001b[0m\n\u001b[0;32m     37\u001b[0m ctx\u001b[39m.\u001b[39minput_params \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(args[length:])\n\u001b[0;32m     38\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 39\u001b[0m     output_tensors \u001b[39m=\u001b[39m ctx\u001b[39m.\u001b[39;49mrun_function(\u001b[39m*\u001b[39;49mctx\u001b[39m.\u001b[39;49minput_tensors)\n\u001b[0;32m     40\u001b[0m \u001b[39mreturn\u001b[39;00m output_tensors\n",
      "File \u001b[1;32mc:\\Users\\finnd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\finnd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\point_e\\models\\transformer.py:79\u001b[0m, in \u001b[0;36mQKVMultiheadAttention.forward\u001b[1;34m(self, qkv)\u001b[0m\n\u001b[0;32m     77\u001b[0m qkv \u001b[39m=\u001b[39m qkv\u001b[39m.\u001b[39mview(bs, n_ctx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     78\u001b[0m q, k, v \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msplit(qkv, attn_ch, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 79\u001b[0m weight \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49meinsum(\n\u001b[0;32m     80\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mbthc,bshc->bhts\u001b[39;49m\u001b[39m\"\u001b[39;49m, q \u001b[39m*\u001b[39;49m scale, k \u001b[39m*\u001b[39;49m scale\n\u001b[0;32m     81\u001b[0m )  \u001b[39m# More stable with f16 than dividing afterwards\u001b[39;00m\n\u001b[0;32m     82\u001b[0m wdtype \u001b[39m=\u001b[39m weight\u001b[39m.\u001b[39mdtype\n\u001b[0;32m     83\u001b[0m weight \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msoftmax(weight\u001b[39m.\u001b[39mfloat(), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mtype(wdtype)\n",
      "File \u001b[1;32mc:\\Users\\finnd\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\functional.py:378\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    373\u001b[0m     \u001b[39mreturn\u001b[39;00m einsum(equation, \u001b[39m*\u001b[39m_operands)\n\u001b[0;32m    375\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(operands) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m opt_einsum\u001b[39m.\u001b[39menabled:\n\u001b[0;32m    376\u001b[0m     \u001b[39m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    377\u001b[0m     \u001b[39m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m--> 378\u001b[0m     \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49meinsum(equation, operands)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    380\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[39mif\u001b[39;00m opt_einsum\u001b[39m.\u001b[39mis_available():\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.64 GiB (GPU 0; 6.00 GiB total capacity; 3.37 GiB already allocated; 650.00 MiB free; 3.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Generate Samples from the Sampler, Using Provided Images\n",
    "\n",
    "samples = None\n",
    "\n",
    "for x in tqdm(sampler.sample_batch_progressive(batch_size=len(images), model_kwargs=dict(images=images))):\n",
    "    samples = x\n",
    "\n",
    "del images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save All Generated Samples To Cloud Files, \n",
    "\n",
    "figs = []\n",
    "\n",
    "for i, cloud in enumerate(sampler.output_to_point_clouds(samples)):\n",
    "    figs[i] = plot_point_cloud(cloud, grid_size=3, fixed_bounds=((-0.75, -0.75, -0.75),(0.75, 0.75, 0.75)))\n",
    "\n",
    "    with open(f'{outputPath}/frags/cloud_{i}.ply', 'wb') as f:\n",
    "        cloud.write_ply(f)\n",
    "\n",
    "del sampler\n",
    "del samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear All Variables from Memory\n",
    "del sampler\n",
    "del samples\n",
    "\n",
    "del figs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Point_E Pointcloud To Mesh</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SDF Generator Model\n",
    "\n",
    "sdfModel = CreateModel('sdf', UseBestTorchDevice())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genreating Meshes From Clouds, Previously Saved To NPZ Files\n",
    "\n",
    "curCloud: PointCloud = None\n",
    "curMesh = None\n",
    "meshes = []\n",
    "\n",
    "for cloudPath in getPathNPBinaries(srcPath):\n",
    "    curCloud = PointCloud.load(cloudPath)\n",
    "\n",
    "    curMesh = marching_cubes_mesh(\n",
    "        pc = curCloud,\n",
    "        model = sdfModel,\n",
    "        batch_size = 4096,\n",
    "        grid_size = 128,\n",
    "        progress=True\n",
    "    )\n",
    "\n",
    "    meshes.append(curMesh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa3b4bcd30999d4dd3b7790820c45ccaef78365348c8c252963eb6d102622af6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
